# Bagging in `scikit-learn`
The two steps we walked through above created trees on bootstrapped samples and randomly selecting features. These can be combined together and implemented at the same time! Combining them adds an additional variation to the base learners for the ensemble model. This in turn increases the ability of the model to generalize to new and unseen data, i.e., it minimizes bias and increases variance. Rather than re-doing this process manually, we will use scikit-learn‘s bagging implementation, BaggingClassifier(), to do so.

Much like other models we have used in scikit-learn, we instantiate a instance of BaggingClassifier() and specify the parameters. The first parameter, base_estimator refers to the machine learning model that is being bagged. In the case of random forests, the base estimator would be a decision tree. We are going to use a decision tree classifier WITH a max_depth of 5, this will be instantiated with BaggingClassifier(DecisionTreeClassifier(max_depth=5)).

After the model has been defined, methods .fit(), .predict(), .score() can be used as expected. Additional hyperparameters specific to bagging include the number of estimators (n_estimators) we want to use and the maximum number of features we’d like to keep (max_features).

Note: While we have focused on decision tress classifiers (as this is the base learner for a random forest classifier), this procedure of bagging is not specific to decision trees, and in fact can be used for any base classifier or regression model. The scikit-learn implementation is generalizable and can be used for other base models!

# Review

A random forest is an ensemble machine learning model. It makes a classification by aggregating the classifications of many decision trees.
Random forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in a random forest is less impactful.
Every decision tree in a random forest is created by using a different subset of data points from the training set. Those data points are chosen at random with replacement, which means a single data point can be chosen more than once. This process is known as bagging.
When creating a tree in a random forest, a randomly selected subset of features are considered as candidates for the best splitting feature. If your dataset has n features, it is common practice to randomly select the square root of n features.